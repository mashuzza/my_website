# Filter data for Multnomah County, starting from March 1, 2020
multnomah_data <- us_cases_clean %>%
filter(Province_State == "Oregon", Admin2 == "Multnomah", date >= as.Date("2020-04-01")) %>%
inner_join(
us_deaths_clean %>%
filter(Province_State == "Oregon", Admin2 == "Multnomah", date >= as.Date("2020-03-01")),
by = c("date", "FIPS", "Admin2")
) %>%
mutate(
lethal_rate = ifelse(cases > 0, (deaths / cases) * 100, NA)
) %>%
filter(!is.na(lethal_rate))  # Remove rows with missing values
# Fit a linear regression model
lethal_rate_model <- lm(lethal_rate ~ date, data = multnomah_data)
# Summarize the model
model_summary <- summary(lethal_rate_model)
# Display the results
tidy(lethal_rate_model)
# Visualize the trend
ggplot(multnomah_data, aes(x = date, y = lethal_rate)) +
geom_point(color = "black", alpha = 0.5) +
geom_smooth(method = "lm", color = "red", se = TRUE) +
labs(
title = "Trend in Lethality Rate for Multnomah County (March 2020 - 2023)",
subtitle = "Linear regression of % deaths among confirmed cases by date",
x = "Date",
y = "Lethality Rate (%)",
caption = "Source: COVID-19 Dataset"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
plot.caption = element_text(size = 10, hjust = 1, color = "gray40"),
axis.text = element_text(size = 12, color = "gray30"),
axis.title = element_text(size = 12, face = "bold", color = "gray30")
)
library(dplyr)
library(ggplot2)
library(broom)
# Filter data for Multnomah County, starting from March 1, 2020
multnomah_data <- us_cases_clean %>%
filter(Province_State == "Oregon", Admin2 == "Multnomah", date >= as.Date("2020-12-01")) %>%
inner_join(
us_deaths_clean %>%
filter(Province_State == "Oregon", Admin2 == "Multnomah", date >= as.Date("2020-03-01")),
by = c("date", "FIPS", "Admin2")
) %>%
mutate(
lethal_rate = ifelse(cases > 0, (deaths / cases) * 100, NA)
) %>%
filter(!is.na(lethal_rate))  # Remove rows with missing values
# Fit a linear regression model
lethal_rate_model <- lm(lethal_rate ~ date, data = multnomah_data)
# Summarize the model
model_summary <- summary(lethal_rate_model)
# Display the results
tidy(lethal_rate_model)
# Visualize the trend
ggplot(multnomah_data, aes(x = date, y = lethal_rate)) +
geom_point(color = "black", alpha = 0.5) +
geom_smooth(method = "lm", color = "red", se = TRUE) +
labs(
title = "Trend in Lethality Rate for Multnomah County (March 2020 - 2023)",
subtitle = "Linear regression of % deaths among confirmed cases by date",
x = "Date",
y = "Lethality Rate (%)",
caption = "Source: COVID-19 Dataset"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
plot.caption = element_text(size = 10, hjust = 1, color = "gray40"),
axis.text = element_text(size = 12, color = "gray30"),
axis.title = element_text(size = 12, face = "bold", color = "gray30")
)
library(dplyr)
library(ggplot2)
library(broom)
# Filter data for Multnomah County, starting from March 1, 2020
multnomah_data <- us_cases_clean %>%
filter(Province_State == "Oregon", Admin2 == "Multnomah", date >= as.Date("2020-12-14")) %>%
inner_join(
us_deaths_clean %>%
filter(Province_State == "Oregon", Admin2 == "Multnomah", date >= as.Date("2020-03-01")),
by = c("date", "FIPS", "Admin2")
) %>%
mutate(
lethal_rate = ifelse(cases > 0, (deaths / cases) * 100, NA)
) %>%
filter(!is.na(lethal_rate))  # Remove rows with missing values
# Fit a linear regression model
lethal_rate_model <- lm(lethal_rate ~ date, data = multnomah_data)
# Summarize the model
model_summary <- summary(lethal_rate_model)
# Display the results
tidy(lethal_rate_model)
# Visualize the trend
ggplot(multnomah_data, aes(x = date, y = lethal_rate)) +
geom_point(color = "black", alpha = 0.5) +
geom_smooth(method = "lm", color = "red", se = TRUE) +
labs(
title = "Trend in Lethality Rate for Multnomah County (March 2020 - 2023)",
subtitle = "Linear regression of % deaths among confirmed cases by date",
x = "Date",
y = "Lethality Rate (%)",
caption = "Source: COVID-19 Dataset"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
plot.caption = element_text(size = 10, hjust = 1, color = "gray40"),
axis.text = element_text(size = 12, color = "gray30"),
axis.title = element_text(size = 12, face = "bold", color = "gray30")
)
lethal_rate_model
model_summary
# Display the results
kable(tidy(lethal_rate_model))
kable(tidy(lethal_rate_model))
kable(tidy(lethal_rate_model))
# Format the regression results with a basic kable
tidy(lethal_rate_model) %>%
kable(
col.names = c("Term", "Estimate", "Std. Error", "t-Statistic", "p-Value"),
caption = "Regression Results for Lethality Rate in Multnomah County",
digits = 4,  # Set decimal places for numeric values
align = "c"  # Center-align columns
)
## get the packages
library(tidyverse)
library(ggplot2)
library(knitr)
library(scales)
library(tigris)
library(sf)
library(broom)
suppressPackageStartupMessages(library(tidyverse))
options(dplyr.summarise.inform = FALSE)  # Suppress dplyr messages
## Load the data
url_in<- 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/'
file_names <- c('time_series_covid19_confirmed_US.csv',
'time_series_covid19_deaths_US.csv')
## concatinate the file names
urls <- str_c(url_in, file_names)
## load the US data
us_cases<-read_csv(urls[1], show_col_types = FALSE)
us_deaths<-read_csv(urls[2], show_col_types = FALSE)
## load Oregon shapefile data
oregon_shapefile <- counties(state = "OR", year = 2020, class = "sf")
options(dplyr.summarise.inform = FALSE)
### US Cases
us_cases_long <- us_cases %>%
pivot_longer(
cols = -c(UID, iso2, iso3, code3, FIPS, Admin2,Province_State,
Country_Region, Lat, Long_, Combined_Key),
names_to = 'date',
values_to = 'cases'
)
## change column format where needed
us_cases_clean <- us_cases_long %>%
mutate(
# Convert `date` to Date format
date = as.Date(date, format = "%m/%d/%y"),
# Convert `Lat` and `Long_` to numeric
Lat = as.numeric(Lat),
Long = as.numeric(Long_),
# Convert categorical columns to factors
iso2 = as.factor(iso2),
iso3 = as.factor(iso3),
Province_State = as.factor(Province_State),
Country_Region = as.factor(Country_Region),
Admin2 = as.factor(Admin2),
Combined_Key = as.factor(Combined_Key),
# Ensure FIPS is numeric and handle NA
FIPS = as.numeric(FIPS),
# Ensure `cases` remains numeric
cases = as.numeric(cases)
)  %>%
# Replace any negative cases
mutate(cases = ifelse(cases < 0, 0, cases))
options(dplyr.summarise.inform = FALSE)
### US Deaths
# Transform the dataset from wide to long format
us_deaths_long <- us_deaths %>%
pivot_longer(
cols = -c(UID, iso2, iso3, code3, FIPS, Admin2, Province_State,
Country_Region, Lat, Long_, Combined_Key),
names_to = 'date',
values_to = 'deaths'
)
# Clean and format the dataset
us_deaths_clean <- us_deaths_long %>%
mutate(
# Convert `date` to Date format
date = as.Date(date, format = "%m/%d/%y"),
# Convert `Lat` and `Long_` to numeric
Lat = as.numeric(Lat),
Long = as.numeric(Long_),
# Convert categorical columns to factors
iso2 = as.factor(iso2),
iso3 = as.factor(iso3),
Province_State = as.factor(Province_State),
Country_Region = as.factor(Country_Region),
Admin2 = as.factor(Admin2),
Combined_Key = as.factor(Combined_Key),
# Ensure FIPS is numeric and handle NA
FIPS = as.numeric(FIPS),
# Ensure `deaths` remains numeric
deaths = as.numeric(deaths)
) %>%
# Replace any negative deaths with 0
mutate(deaths = ifelse(deaths < 0, 0, deaths)) %>%
# Remove rows where `date` is NA
filter(!is.na(date))
# Plot the heatmap
ggplot(oregon_map_data) +
geom_sf(aes(fill = lethal_rate, geometry = geometry), color = "white", size = 0.2) +
facet_wrap(~ year) +
scale_fill_gradient(low = "white", high = "black", na.value = "lightblue", name = "Lethal Rate (%)") +
labs(
title = "Lethal Case Rate by County in Oregon",
subtitle = "Visualizing % of deaths among confirmed cases by year",
caption = "Source: COVID-19 Dataset",
x = NULL,
y = NULL
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
plot.caption = element_text(size = 10, hjust = 1, color = "gray40"),
axis.text = element_blank(),
axis.title = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
strip.text = element_text(size = 12, face = "bold", color = "gray30"),
legend.position = "right",
legend.title = element_text(size = 12, face = "bold", color = "gray30"),
legend.text = element_text(size = 10, color = "gray30")
)
library(tidyverse)
library(ggplot2)
library(knitr)
library(forecast)
library(broom)
url_name1<- 'https://data.cityofnewyork.us/api/views/833y-fsy8/rows.csv?accessType=DOWNLOAD'
nypd_data<-read_csv(url_name1, show_col_types = FALSE)
nypd_data_clean <- nypd_data %>%
# Replace (null) and UNKNOWN values with NA
mutate(
across(
where(is.character),
~ ifelse(.x %in% c("(null)", "UNKNOWN"), NA, .x)
)
) %>%
# Replace odd or invalid values with NA
mutate(
PERP_AGE_GROUP = ifelse(PERP_AGE_GROUP %in% c("1020", "1028", "224", "940"),
NA, as.character(PERP_AGE_GROUP)),
VIC_AGE_GROUP = ifelse(VIC_AGE_GROUP %in% c("1022"),
NA, as.character(VIC_AGE_GROUP)),
PERP_SEX = ifelse(PERP_SEX %in% c("U"),
NA, as.character(PERP_SEX)),
VIC_SEX = ifelse(VIC_SEX %in% c("U"),
NA, as.character(VIC_SEX)),
LOCATION_DESC = ifelse(LOCATION_DESC %in% c("NONE"),
NA, as.character(LOCATION_DESC))
) %>%
# Format and recode variables
mutate(
OCCUR_DATE = as.Date(OCCUR_DATE, format = "%m/%d/%Y"),
BORO = as.factor(BORO),
JURISDICTION = recode(
JURISDICTION_CODE,
`0` = "Patrol",
`1` = "Transit",
`2` = "Housing"
),
JURISDICTION = as.factor(JURISDICTION),
LOC_OF_OCCUR_DESC = as.factor(LOC_OF_OCCUR_DESC),
PERP_AGE_GROUP = factor(PERP_AGE_GROUP,
levels = c("<18", "18-24", "25-44", "45-64", "65+")),
PERP_SEX = as.factor(PERP_SEX),
PERP_RACE = as.factor(PERP_RACE),
VIC_AGE_GROUP = factor(VIC_AGE_GROUP,
levels = c("<18", "18-24", "25-44", "45-64", "65+")),
VIC_SEX = as.factor(VIC_SEX),
VIC_RACE = as.factor(VIC_RACE),
LOC_CLASSFCTN_DESC = as.factor(LOC_CLASSFCTN_DESC),
LOCATION_DESC = as.factor(LOCATION_DESC)
)
# Create additional variables
nypd_data_clean <- nypd_data_clean %>%
mutate(
OCCUR_MONTH = as.Date(floor_date(OCCUR_DATE))
)
# Calculate percentage of incidents by borough and hour
nypd_percentage <- nypd_data_clean %>%
mutate(
HOUR = hour(hms(OCCUR_TIME))  # Extract the hour from OCCUR_TIME
) %>%
group_by(BORO, HOUR) %>%  # Group by borough and hour
summarise(
INCIDENTS = n(),  # Count the number of incidents
.groups = "drop"
) %>%
group_by(BORO) %>%  # Group by borough to calculate percentages
mutate(
PERCENT = INCIDENTS / sum(INCIDENTS) * 100  # Calculate percentage within each borough
)
# Manually reorder BORO from largest to smallest population
nypd_percentage <- nypd_percentage %>%
mutate(BORO = factor(BORO, levels = c("BROOKLYN", "QUEENS", "MANHATTAN",
"BRONX", "STATEN ISLAND")))
ggplot(nypd_percentage, aes(x = HOUR, y = BORO, fill = PERCENT)) +
geom_tile(color = "white") +  # Heatmap with white borders for separation
scale_fill_gradient(low = "white", high = "gray10", name = "Percent (%)") +
labs(
title = "Heatmap of Shooting Incidents by Borough and Time of Day",
subtitle = "Percentage distribution of incidents by hour within each borough",
x = "Hour of Day", y = "Borough",
caption = "Source: NYPD Shooting Incident Dataset"
) +
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
plot.caption = element_text(size = 10, hjust = 1, color = "gray40"),
axis.text = element_text(size = 12, color = "gray30"),
axis.title = element_text(size = 12, face = "bold", color = "gray30"),
panel.grid.major = element_blank(),  # Minimal gridlines
panel.grid.minor = element_blank(),
plot.margin = margin(20, 20, 20, 20)
)
# Aggregate data by month
incident_trends <- nypd_data_clean %>%
mutate(YEAR_MONTH = floor_date(OCCUR_DATE, "month")) %>%
group_by(YEAR_MONTH) %>%
summarise(INCIDENTS = n())
# visualize the trend
ggplot(incident_trends, aes(x = YEAR_MONTH, y = INCIDENTS)) +
# Main line plot for trends
geom_line(color = "black", size = 1.2, alpha = 0.8) +
# Smoothed trend line
geom_smooth(method = "loess", color = "gray30", fill = "gray70",
se = TRUE, size = 1, alpha = 0.3) +
# Titles and labels
labs(
title = "Trend of NYC Shooting Incidents",
subtitle = "Monthly shooting incidents from 2006 to 2023",
x = NULL,
y = "Number of Shooting Incidents",
caption = "Source: NYPD Shooting Incident Dataset"
) +
# NY Times-inspired theme
theme_minimal(base_size = 14) +
theme(
plot.title = element_text(face = "bold", size = 18, hjust = 0.5),
plot.subtitle = element_text(size = 12, hjust = 0.5, margin = margin(b = 10)),
plot.caption = element_text(size = 10, hjust = 1, color = "gray40"),
axis.text = element_text(size = 12, color = "gray30"),
axis.title = element_text(size = 12, face = "bold", color = "gray30"),
panel.grid.major = element_line(color = "gray90", size = 0.5),
panel.grid.minor = element_blank(),
plot.margin = margin(20, 20, 20, 20)
) +
# Adjust x-axis for NYT-style clarity
scale_x_date(
date_labels = "%Y",
date_breaks = "2 years",
expand = expansion(mult = c(0.01, 0.01))
) +
# Adjust y-axis to start from 0 for clarity
scale_y_continuous(
expand = expansion(mult = c(0.01, 0.01))
)
# Convert the incident_trends data into a time series object
shooting_ts <- ts(incident_trends$INCIDENTS,
start = c(year(min(incident_trends$YEAR_MONTH)),
month(min(incident_trends$YEAR_MONTH))),
frequency = 12)  # Monthly data
# Fit an ARIMA model to the time series
arima_model <- auto.arima(shooting_ts)
# Forecast the next 12 months
forecasted_incidents <- forecast(arima_model, h = 12)
arima_model
tidy_arima
# Convert the incident_trends data into a time series object
shooting_ts <- ts(incident_trends$INCIDENTS,
start = c(year(min(incident_trends$YEAR_MONTH)),
month(min(incident_trends$YEAR_MONTH))),
frequency = 12)  # Monthly data
# Fit an ARIMA model to the time series
arima_model <- auto.arima(shooting_ts)
# Forecast the next 12 months
forecasted_incidents <- forecast(arima_model, h = 12)
# Tidy the ARIMA model object
tidy_arima <- tidy(arima_model)
# Display the results as a well-formatted table using kable
tidy_arima %>%
kable(
col.names = c("Term", "Estimate", "Std. Error", "t-Statistic", "p-Value"),
caption = "Regression Results for ARIMA Model",
digits = 4,  # Set decimal places for numeric values
align = "c"  # Center-align columns
)
tidy_arima
(82.17-81)/(2.2/sqrt(12))
qnrom(1-0.975)
qnorm(1-0.975)
qnorm(0.975)
z = (82.17-81)/(2.2/sqrt(12))
# Calculate the two-tailed p-value
p_value <- 2 * (1 - pnorm(abs(z)))
p_value
0.6*500
0.35*500
0.05*500
# Observed frequencies
observed <- c(292, 190, 18)
# Sample size
n <- 500
# Expected proportions
expected_proportions <- c(0.60, 0.35, 0.05)
# Expected frequencies
expected <- expected_proportions * n
# Perform chi-squared test
chi_squared_test <- chisq.test(x = observed, p = expected_proportions, rescale.p = TRUE)
# Extract p-value
p_value <- chi_squared_test$p.value
chi_squared_test
t.test
t_stat = (3.87-4.69)/(1.45/sqrt(15))
t_stat
pt(t_stat)
pt(t_stat, df = 15-1)
qt(1-0.1)
qt(1-0.1, df = 14)
qt(0.1, df = 14)
n = 200
p1_hat = 167/n
p2_hat = 178/n
# Data
n1 <- 200  # Sample size for Population 1
x1 <- 167  # Number of successes in Population 1
n2 <- 200  # Sample size for Population 2
x2 <- 178  # Number of successes in Population 2
# Sample proportions
p1_hat <- x1 / n1
p2_hat <- x2 / n2
# Pooled proportion
p_hat <- (x1 + x2) / (n1 + n2)
# Test statistic
z <- (p1_hat - p2_hat) / sqrt(p_hat * (1 - p_hat) * (1/n1 + 1/n2))
# One-tailed p-value (H1: p1 < p2)
p_value <- pnorm(z)
# Output results
cat("Z-statistic:", round(z, 3), "\n")
cat("P-value:", round(p_value, 3), "\n")
pchisq(1-0.05,14)
pchisq(0.05,14)
qchisq(1-0.05,14)
qchisq(0.05,14)
qchisq(1-0.05,14)
1.45^2*qchisq(1-0.05,14)/14
1.45^2
2^2*qchisq(1-0.05,14)/14
2*qchisq(1-0.05,14)/14
qf(0.05/2,3-1,5-1)
pf(f<-qf(0.05/2,3-1,5-1))
pf(7.8/6.3, 3-1, 5-1)
qf(0.05/2,3-1,5-1)
7.8/6.3
p_value<-pf(7.8/6.3, 3-1, 5-1)
p_value
qf(0.05/2,3-1,5-1)
qf(1-0.05,3-1,5-1)
qchisq(1-0.08,1)
(26-22)^2/22
(26-22)^2/22 + (18-22)^2/22 + (29-33)^2/33 +(37-33)^2/33
# Degrees of freedom
df1 <- 4
df2 <- 2
# Significance level
alpha <- 0.05
# Lower-tail critical value
critical_value <- qf(alpha, df1, df2)
# Output result
cat("Critical value:", round(critical_value, 3), "\n")
install.packages(c("bit", "class", "cluster", "cpp11", "curl", "data.table", "emmeans", "fontawesome", "KernSmooth", "knitr", "lubridate", "MASS", "nnet", "openssl", "pillar", "RcppArmadillo", "rmarkdown", "spatial", "survival", "textshaping"))
quarto create-project my-website --type website
install.packages("postcards")
create_postcard("jolla")
library(postcards)
create_postcard("jolla")
help(postcards)
library(postcards)
help(postcards)
sessionInfo()
library(postcards)
?create_postcard
postcards::create_postcard(template = "jolla")
getwd()
setwd("~/ML Website")
getwd()
list.files()
1.7813+1.8693
round(1.7813+1.8693,2)
1.7813 +2.4594+1.8693*2-1.2221
